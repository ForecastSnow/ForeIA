# PUERTO DE ESCUCHA

PORT = 3000

# ES IMPORTANTE COLOCAR LA MISMA KEY QUE EL SERVIDOR PRINCIPAL QUE VA HACER USO DE ESTE MICROSERVICIO

FOREIANUCLE_MICROSERVICE_TOKENKEY = "la misma contrase√±a que en el api principal"

# ES POSIBLE CORRER OTROS MODELOS DE IA SUMINISTRANDO EL ID DE UN MODELO DE HUGGING FACE SIN EMBARGO
# EL CODIGO ESTA PROBADO Y OPTIMIZADO PARA CORRER "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16"
# https://huggingface.co/context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16

MODEL_TEXT_ID = "context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16"

# ==quantization config==
# "4" 4bit
# "8" 8bit

QUANTIZATION_CONFIG = 8

# IMPORTANTE ENTENDER QUE LA QUANTIZATION ES UNA TECNICA QUE REDUCE EL CONSUMO DE VRAM A CAMBIO DE PRECISION EN EL MODELO
# NORMALMENTE EN 8 BITS NO ES TAN AGRESIVO, 4 BITS REDUCE AGRESIVAMENTE EL CONSUMO DE RAM PERO LA PERDIDA
# DE PRECISION ES MUY NOTORIA.


# TOKEN DEL TUNNEL DE CLOUDFLARE.

CLOUDFLARED_TUNNEL_TOKEN = "debes colocar el token del tunnel que creaste en tu plataforma de cloudflare"
