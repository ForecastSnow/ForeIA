# ğŸ§  IANucle â€“ Microservicio de IA para ForeIA

**IANucle** es el microservicio destinado a gestionar la inteligencia artificial dentro del proyecto. **ForeIA** una demo tecnica inspirada en ChatGPT/Gemini.  
Este componente se encarga exclusivamente de **generar y procesar texto** utilizando un modelo de lenguaje local, optimizado para correr en hardware modesto con tÃ©cnicas de **cuantizaciÃ³n a 8 bits**.

**Nota del Autor:** Este proyecto fue diseÃ±ado, desarrollado e implementado en su totalidad por **una sola persona**, demostrando una completa comprensiÃ³n de la arquitectura de microservicios, optimizaciÃ³n de LLM y despliegue seguro.

âš¡ DiseÃ±ado para demostrar cÃ³mo una arquitectura profesional de IA puede funcionar incluso con recursos limitados.

---

## âœ¨ CaracterÃ­sticas principales

- ğŸš€ **Microservicio independiente**: diseÃ±ado en **Flask**, desplegado en producciÃ³n con **Gunicorn**.  
- ğŸ§© **IntegraciÃ³n vÃ­a orquestador**: IANucle no expone lÃ³gica de negocio; solo responde a peticiones del backend principal de ForeIA.  
- ğŸ¤– **Modelo local**: `Llama 3.2 Instruct 3B FP16` cuantizado a 8 bits, servido desde https://huggingface.co/context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16.  
- ğŸ”’ **Seguridad por tokens JWT**: el orquestador genera y valida tokens; IANucle solo atiende requests autenticados.  
- ğŸŒ **Despliegue en entorno real**: servidor casero, con tÃºneles seguros vÃ­a **Cloudflare Zero Trust**.  
- âš™ï¸ **OptimizaciÃ³n** con `transformers`, `bitsandbytes`, `accelerate` y `optimum` para correr en hardware limitado.

---

## âš™ï¸ Stack tecnolÃ³gico

- **Lenguaje:** Python 3  
- **Framework:** Flask + Gunicorn  
- **IA & NLP:** Transformers, Accelerate, BitsAndBytes, Optimum  
- **Infraestructura:** Cloudflare Tunnel (Zero Trust)  
- **AutenticaciÃ³n:** JWT firmado desde orquestador  
- **Hardware objetivo:** (6GB VRAM)

---

## ğŸ”Œ Endpoints principales

Todos los endpoints responden en **JSON**.  
El middleware espera un **token JWT vÃ¡lido en `request.cookies["Authorization"]`**.

| MÃ©todo | Endpoint                  | DescripciÃ³n                                                                          |
| ------ | ------------------------- | ------------------------------------------------------------------------------------ |
| `GET`  | `/textiastatus`           | Estado del microservicio de IA.                                                      |
| `POST` | `/message`                | Genera una respuesta del modelo segÃºn el `message` y el historial de `conversation`. |
| `POST` | `/chatNameGenerator`      | Genera un nombre para el chat basÃ¡ndose en la conversaciÃ³n.                          |
| `POST` | `/chatResume`             | Genera un resumen de la conversaciÃ³n.                                                |
| `GET`  | `/foreianucle/api/status` | Respuesta bÃ¡sica de estado: `{"message": "hello there"}`.                            |



## ğŸ—ï¸ Arquitectura en contexto

IANucle no vive aislado: es uno de los microservicios que conforman **ForeIA**.  

flowchart = [Frontend] --> [Orquestador API] --> [IANucle - Microservicio de IA] --> [Modelo Llama 3.2 Instruct 3B]

---

## ğŸ“š Roadmap

- ğŸ”§ Soporte para mÃºltiples modelos configurables por variable de entorno.   
- âš¡ Inferencia optimizada con ONNX Runtime.  
- ğŸ” gestiÃ³n de tokens.

---

## ğŸ“„ Licencia

El codigo fuente del proyecto se distribuye bajo la **MIT License**.  
El modelo base que se utiliza: *[Llama 3.2 Instruct 3B](https://huggingface.co/context-labs/meta-llama-Llama-3.2-3B-Instruct-FP16.)*, tiene su propia licencia y debe ser respetada segun los terminos de META.  

---

ğŸ’¡ *IANucle demuestra cÃ³mo es posible diseÃ±ar un microservicio de IA escalable y seguro incluso en un entorno de recursos limitados.*

---

## ğŸ“¦ Instrucciones de despliegue

Este microservicio estÃ¡ preparado para correr en contenedores **Docker**, con soporte para **docker-compose** y configuraciÃ³n mediante **.env**.

### 1ï¸âƒ£ Configurar variables de entorno

- Copiar el archivo `env-example` y renombrarlo a `.env`.
- Completar los valores necesarios.  
- La variable mÃ¡s importante es el **token de conexiÃ³n de Cloudflare Tunnel**, que permite exponer el servicio de manera segura.  
- El resto de las variables ya tienen instrucciones en el archivo `env-example`.

### 2ï¸âƒ£ Construir la imagen y levantar con docker-compose

- Ejecutar en la raÃ­z del proyecto: docker-compose up -d


- El microservicio quedarÃ¡ operativo y accesible a travÃ©s del tÃºnel de Cloudflare Zero Trust.

